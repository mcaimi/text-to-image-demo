{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc187e7e-ab1c-43f6-bf7c-ff741bc8c797",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference to a KServe v1 Predictor\n",
    "\n",
    "Let's deploy our fine tuned model via KServe using the [v1 API](https://kserve.github.io/website/latest/modelserving/data_plane/v1_protocol/).\n",
    "A custom model server can be found [here](https://quay.io/repository/marcocaimi/kserve-diffusers) and its source code can be found [here](https://github.com/mcaimi/kserve-diffusers-demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab32594-7d7d-410f-a059-e4ed7734e38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install requests pillow\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bb5e2-5646-4f0a-b842-e0628261dfff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "try:\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import io\n",
    "except Exception as e:\n",
    "    print(f\"Caught Exception {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c004acc-13cd-4917-8480-592c7c2d623b",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Change that following variable settings match your deployed model's *Inference endpoint*. for example: \n",
    "\n",
    "```\n",
    "infer_endpoint = \"https://sd-stable-diffusion.apps.cluster-xc4f6.sandbox942.opentlc.com\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de65d02-84a6-4cff-882e-551cdd42b486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the inference endpoint exposed by the model server\n",
    "# this actually is the entry point that openshift exposes to hide the containerized workload\n",
    "# KServe is serverless actually\n",
    "infer_endpoint = \"http://localhost:8080\"\n",
    "\n",
    "# this is the inference method exposed by the KServe Model Server\n",
    "infer_url = f\"{infer_endpoint}/v1/models/model:predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f9ece-e9cf-44e2-a8a2-73160186aee8",
   "metadata": {},
   "source": [
    "## Request Function\n",
    "\n",
    "Build and submit the REST request to the model server. \n",
    "\n",
    "An example JSON payload:\n",
    "\n",
    "```json\n",
    " // example payload:\n",
    " {\n",
    "   \"instances\": [\n",
    "     {\n",
    "       \"prompt\": \"photo of the beach\",\n",
    "       \"negative_prompt\": \"ugly, deformed, bad anatomy\",\n",
    "       \"num_inference_steps\": 20,\n",
    "       \"width\": 512,\n",
    "       \"height\": 512,\n",
    "       \"guidance_scale\": 7,\n",
    "       \"scheduler\": \"DPM++ 2M\",\n",
    "       \"seed\": 772847624537827,\n",
    "     }\n",
    "   ]\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9386f-683a-4880-b780-c40bec3ab9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the call function\n",
    "def rest_request(url, prompt,\n",
    "                 negative_prompt=\"\",\n",
    "                 steps=10,\n",
    "                 width=512, height=512,\n",
    "                 cfg=7,\n",
    "                 scheduler=\"DPM++ 2M\",\n",
    "                 seed=-1,\n",
    "                 timeout=600,\n",
    "                 tls_verify=False):\n",
    "    # prepare payload\n",
    "    json_data = {\n",
    "        \"instances\": [\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"negative_prompt\": negative_prompt,\n",
    "                \"num_inference_steps\": steps,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"guidance_scale\": cfg,\n",
    "                \"scheduler\": scheduler,\n",
    "                \"seed\": seed,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # call the inference service\n",
    "    response = requests.post(url, json=json_data, verify=tls_verify, timeout=timeout)\n",
    "\n",
    "    # extract the resoponse payload\n",
    "    response_dict = response.json()\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c0207-8896-420d-bbbc-29fc1d489f55",
   "metadata": {},
   "source": [
    "## Call the remote inference server\n",
    "\n",
    "Let's call KServe to generate a RHTeddy image for us. Make sure the inference server has at least a GPU available otherwise this wwill most likey fail due to timeouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e6e0b-ad2e-40b5-a0d3-752447027350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt\n",
    "prompt = \"a photo of rhteddy dog wearing his red fedora on in the snowy mountains\"\n",
    "\n",
    "# call the service\n",
    "json_response = rest_request(infer_url, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229054e5-1f44-413c-a726-64f5e973f8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now extract the image from the payload\n",
    "# the image is base64 encoded\n",
    "img_str = json_response[\"predictions\"][0][\"image\"][\"b64\"]\n",
    "img_data = base64.b64decode(img_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66e0f7-4d4e-4879-bdf1-36b712432fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the image back from base64 and display it\n",
    "image = Image.open(io.BytesIO(img_data))\n",
    "image.save(\"teddy.png\", format=\"PNG\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbb97c-6cfb-45d2-8e8d-334bb076bab0",
   "metadata": {},
   "source": [
    "## Build a Stable Diffusion UI with Gradio\n",
    "\n",
    "Let's try to make our inference endpoint callable from a pretty web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85509c8b-cb04-4927-b424-bbc2a9365bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the gradio toolkit\n",
    "try:\n",
    "    import gradio as gr\n",
    "except Exception as e:\n",
    "    print(f\"Unable to import gradio library: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f816c-0ff3-4ed1-b58b-727ca29eed50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the callback function\n",
    "def generate_image(url, prompt,\n",
    "                 negative_prompt=\"\",\n",
    "                 steps=10,\n",
    "                 width=512, height=512,\n",
    "                 cfg=7,\n",
    "                 scheduler=\"DPM++ 2M\",\n",
    "                 seed=-1,\n",
    "                 timeout=600,\n",
    "                 tls_verify=False):\n",
    "    # call the generation function\n",
    "    kserve_response = rest_request(url, prompt, negative_prompt=negative_prompt,\n",
    "                                   steps=steps, width=width, height=height,\n",
    "                                   cfg=cfg, seed=seed, scheduler=scheduler, timeout=timeout, tls_verify=tls_verify)\n",
    "\n",
    "    # extract the payload\n",
    "    image_payload = kserve_response.get(\"predictions\")[0].get(\"image\").get(\"b64\")\n",
    "    # decode from base64\n",
    "    img_data = base64.b64decode(image_payload)\n",
    "\n",
    "    # return image bytes\n",
    "    return Image.open(io.BytesIO(img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e082dcd-f50c-47ea-9984-1bc72385b9a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build gradio application\n",
    "schedulers = [\"DPM++ 2M\", \"DPM++ SDE\", \"Euler a\", \"Euler\", \"Heun\", \"LCM\"]\n",
    "\n",
    "sd_ui = gr.Interface(fn=generate_image,\n",
    "                     inputs=[gr.Textbox(value=infer_url, label=\"Inference URL\"),\n",
    "                            gr.Textbox(label=\"Prompt\"),\n",
    "                            gr.Textbox(label=\"Negative Prompt\"),\n",
    "                            gr.Slider(label=\"Denoising Steps\", value=5, minimum=1, maximum=100, step=1),\n",
    "                            gr.Number(label=\"Width\", value=512), gr.Number(label=\"Height\", value=512),\n",
    "                            gr.Slider(label=\"Guidance Scale\", value=7, minimum=1, maximum=100, step=0.5),\n",
    "                            gr.Dropdown(label=\"Scheduler\", value=\"DPM++ 2M\", choices=schedulers), gr.Number(value=-1, label=\"Seed\")],\n",
    "                     outputs=gr.Image())\n",
    "\n",
    "# start the application\n",
    "sd_ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf7b61-509d-49a0-a330-e5a03faded13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# close the application and exit\n",
    "sd_ui.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdeea8f-74ed-4bfc-bd0d-cebe7fe70022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
